# SimplicityPress Static Search Specification

> **Goal:** Provide a fast, fully static search feature for sites generated by SimplicityPress.
>
> **Non-goal:** Do not add any server-side dependencies or runtime indexing. Search must work on any static host.

## Summary

SimplicityPress generates a search index **at build time** and outputs:

- A **search page** (theme template)
- One or more **JSON index artifacts**
- A small **client-side script** that performs query and ranking in the browser

Search is a *build artifact*:
> content → SimplicityPress → static site (+ search assets)

---

## Output Files

Default output paths (suggested):

- `assets/search/search_docs.json`
- `assets/search/search_terms.json`
- `assets/search/search.js`
- `search/index.html` (or `/search.html`)

> The exact output path should be configurable, but defaults should “just work.”

---

## Configuration

Add configuration keys (names can be adjusted to match existing config patterns):

```yaml
search:
  enabled: true
  output_dir: "assets/search"
  page_path: "search/index.html"

  # Index tuning
  max_terms_per_doc: 300
  min_token_len: 2
  drop_df_ratio: 0.70     # drop terms in >= 70% of docs
  drop_df_min: 2          # optionally drop ultra-rare terms (optional; default off if 0)

  # Weighting
  weight_body: 1.0
  weight_title: 8.0
  weight_tags: 6.0

  # Normalization
  normalize_by_doc_len: true
````

If `search.enabled = false`, **no search assets** are written.

---

## Document Model (Index Input)

Each rendered page/post must provide:

- `url` (output url/path)
- `title` (string; fallback derived from filename if missing)
- `tags` (list; default empty)
- `date` (optional string; ISO-8601 recommended)
- `excerpt` (short text snippet; recommended 160–240 chars)
- `content_text` (plain text version of Markdown body)

> `content_text` is used only for tokenization at build time; it is not necessarily stored in the output JSON (unless desired).

---

## JSON Artifact: `search_docs.json`

Purpose: UI metadata for results display.

Schema:

```json
{
  "version": 1,
  "generated_at": "2025-12-13T00:00:00Z",
  "doc_count": 123,
  "docs": [
    {
      "id": 1,
      "url": "/posts/my-post.html",
      "title": "My Post Title",
      "tags": ["tag1", "tag2"],
      "date": "2025-12-01",
      "excerpt": "First ~200 characters of the content..."
    }
  ]
}
```

Rules:

- `id` is an integer 0..N-1 (or 1..N), stable for this build.
- `url` should match the site’s output structure.
- `excerpt` should be plain text (no HTML).

---

## JSON Artifact: `search_terms.json`

Purpose: compact inverted index for query evaluation.

Schema:

```json
{
  "hippo": [[1, 3.42], [9, 1.10]],
  "wordpress": [[1, 4.90]]
}
```

Rules:

- Keys are **tokens** (lowercased).
- Values are arrays of `[doc_id, score]`.
- Postings may be stored **sorted by score descending** (recommended).

This file should remain reasonably small by:

- dropping high-DF terms
- limiting stored terms per document to the top N by score

---

## Tokenization and Text Extraction

### Plain text extraction

- Convert Markdown to plain text (strip markup).
- Ignore HTML tags if present.
- Recommended: treat headings as part of text, but title gets its own weight anyway.

### Tokenization

- Lowercase
- Split on non-alphanumeric boundaries
- Keep `a-z0-9` words by default (optionally allow unicode letters later)
- Drop tokens shorter than `min_token_len`

Suggested regex approach:

- Find tokens via something like: `r"[A-Za-z0-9]+"` (v1)
- (Future) optional unicode-aware tokenization

---

## Scoring

This is a build-time TF-IDF style score with optional normalization.

### Step 1: weighted term counts per document

For each document:

- Count occurrences in:

  - body text (`weight_body`)
  - title (`weight_title`)
  - tags (`weight_tags`)

`count(term, doc) =
  weight_body - body_hits +
  weight_title - title_hits +
  weight_tags - tag_hits`

### Step 2: term frequency transform (TF)

Use a capped/log transform to prevent long documents dominating.

Recommended:

- `tf = 1 + ln(count)` for `count > 0`

### Step 3: document frequency (DF)

- `df(term) = number of documents where term appears at least once`

Drop rules:

- Drop if `df == N`
- Drop if `(df / N) >= drop_df_ratio` (default 0.70)

(Optional) also drop ultra-rare if desired:

- Drop if `df < drop_df_min` and `drop_df_min > 0`

### Step 4: inverse document frequency (IDF)

Recommended:

- `idf = ln((N + 1) / (df + 1)) + 1`

### Step 5: final per-term score

- `score = tf * idf`

### Optional Step 6: document length normalization

If enabled:

- `score = score / sqrt(doc_token_count)` (doc_token_count from body tokens)

---

## Index Size Controls

To keep payload small and performance stable:

- For each document, keep only the **top `max_terms_per_doc`** terms by `score`
- Then build the inverted index from those retained terms

Recommended defaults:

- `max_terms_per_doc = 300`
- `drop_df_ratio = 0.70`
- `min_token_len = 2`

---

## Client-Side Query Behavior

### Query parsing

- Lowercase
- Tokenize using same rules as build
- Remove tokens shorter than `min_token_len`

### Ranking

For each query token:

- retrieve postings list
- accumulate doc scores:

  - `doc_score[doc_id] += posting_score`

Sort by `doc_score` descending.

### Rendering

Use `search_docs.json` to render:

- Title → link to URL
- Excerpt
- Tags / date (optional)

---

## Incremental Implementation Plan

### Phase 1: Safe scaffolding

- Add config flag `search.enabled` (default false)
- Add a stub search page template and empty JSON generation (no impact on existing builds)

### Phase 2: Generate docs + naive index

- Generate `search_docs.json` from existing content metadata
- Generate `search_terms.json` from title + tags only (small and safe)
- Add minimal `search.js` to search those tokens

### Phase 3: Full text indexing

- Extract plain text from Markdown bodies
- Implement TF/DF/IDF scoring + DF-drop rules
- Add top-term-per-doc trimming

### Phase 4: Performance polish (optional)

- Sort postings
- Optionally shard terms JSON by first letter if payload grows

At each phase, existing pipeline remains valid and search can be toggled off.

---

## Testing

Add unit tests for:

- tokenization behavior
- DF drop rules (df==N, df/N>=threshold)
- scoring monotonicity (rarer terms score higher than common terms)
- top-terms-per-doc trimming
- stable JSON schema output

Add a small fixture site in tests with known content to assert expected results order.
